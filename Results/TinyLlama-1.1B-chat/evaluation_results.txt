tinyllama-1.1B-chat: Evaluation results of: BD Scene 1: {'rouge_score_': {'rouge1_fmeasure': tensor(0.3328), 'rouge1_precision': tensor(0.3741), 'rouge1_recall': tensor(0.3564), 'rougeL_fmeasure': tensor(0.3253), 'rougeL_precision': tensor(0.3655), 'rougeL_recall': tensor(0.3487)}, 'bertscore_': {'precision': 0.7996399257458797, 'recall': 0.7926018056415376, 'f1': 0.7932694684891474, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.41.2)'}}

Evaluation results of: BD Scene 2: {'rouge_score_': {'rouge1_fmeasure': tensor(0.3425), 'rouge1_precision': tensor(0.3847), 'rouge1_recall': tensor(0.3819), 'rougeL_fmeasure': tensor(0.3294), 'rougeL_precision': tensor(0.3693), 'rougeL_recall': tensor(0.3688)}, 'bertscore_': {'precision': 0.7767835809239025, 'recall': 0.775110758969496, 'f1': 0.7733156620685396, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.41.2)'}}

Evaluation results of: HC Scene 1: {'rouge_score_': {'rouge1_fmeasure': tensor(0.3062), 'rouge1_precision': tensor(0.3446), 'rouge1_recall': tensor(0.3429), 'rougeL_fmeasure': tensor(0.3010), 'rougeL_precision': tensor(0.3385), 'rougeL_recall': tensor(0.3380)}, 'bertscore_': {'precision': 0.7971179099970086, 'recall': 0.7901282953661541, 'f1': 0.7906160479368165, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.41.2)'}}

Evaluation results of: HC Scene 2: {'rouge_score_': {'rouge1_fmeasure': tensor(0.3637), 'rouge1_precision': tensor(0.3911), 'rouge1_recall': tensor(0.3696), 'rougeL_fmeasure': tensor(0.3436), 'rougeL_precision': tensor(0.3695), 'rougeL_recall': tensor(0.3488)}, 'bertscore_': {'precision': 0.7918936231617252, 'recall': 0.7843308812047279, 'f1': 0.7870701712600151, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.41.2)'}}

Evaluation results of: SZ Scene 1: {'rouge_score_': {'rouge1_fmeasure': tensor(0.3007), 'rouge1_precision': tensor(0.3560), 'rouge1_recall': tensor(0.3222), 'rougeL_fmeasure': tensor(0.2926), 'rougeL_precision': tensor(0.3462), 'rougeL_recall': tensor(0.3147)}, 'bertscore_': {'precision': 0.7949179043953724, 'recall': 0.7787486414643044, 'f1': 0.7836224823556048, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.41.2)'}}

Evaluation results of: SZ Scene 2: {'rouge_score_': {'rouge1_fmeasure': tensor(0.3717), 'rouge1_precision': tensor(0.4033), 'rouge1_recall': tensor(0.4030), 'rougeL_fmeasure': tensor(0.3597), 'rougeL_precision': tensor(0.3896), 'rougeL_recall': tensor(0.3904)}, 'bertscore_': {'precision': 0.7898012597909134, 'recall': 0.7881208105033704, 'f1': 0.7868920599476675, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.41.2)'}}


Evaluation results of: BD Scene 1: {'rouge_score_': {'rouge1_fmeasure': tensor(0.3330), 'rouge1_precision': tensor(0.3742), 'rouge1_recall': tensor(0.3567), 'rougeL_fmeasure': tensor(0.3254), 'rougeL_precision': tensor(0.3655), 'rougeL_recall': tensor(0.3488)}, 'bertscore_': {'precision': 0.7996614628097638, 'recall': 0.7926407187163425, 'f1': 0.7932988731228575, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: BD Scene 2: {'rouge_score_': {'rouge1_fmeasure': tensor(0.3425), 'rouge1_precision': tensor(0.3846), 'rouge1_recall': tensor(0.3823), 'rougeL_fmeasure': tensor(0.3294), 'rougeL_precision': tensor(0.3693), 'rougeL_recall': tensor(0.3692)}, 'bertscore_': {'precision': 0.7766317944845249, 'recall': 0.775244223146603, 'f1': 0.7732817276284613, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: HC Scene 1: {'rouge_score_': {'rouge1_fmeasure': tensor(0.3062), 'rouge1_precision': tensor(0.3446), 'rouge1_recall': tensor(0.3429), 'rougeL_fmeasure': tensor(0.3010), 'rougeL_precision': tensor(0.3385), 'rougeL_recall': tensor(0.3380)}, 'bertscore_': {'precision': 0.7971179087494695, 'recall': 0.7901282925938451, 'f1': 0.7906160479368165, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: HC Scene 2: {'rouge_score_': {'rouge1_fmeasure': tensor(0.3640), 'rouge1_precision': tensor(0.3913), 'rouge1_recall': tensor(0.3699), 'rougeL_fmeasure': tensor(0.3439), 'rougeL_precision': tensor(0.3697), 'rougeL_recall': tensor(0.3491)}, 'bertscore_': {'precision': 0.7919540405273438, 'recall': 0.7844059569641244, 'f1': 0.78713809650855, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: SZ Scene 1: {'rouge_score_': {'rouge1_fmeasure': tensor(0.3007), 'rouge1_precision': tensor(0.3560), 'rouge1_recall': tensor(0.3222), 'rougeL_fmeasure': tensor(0.2926), 'rougeL_precision': tensor(0.3462), 'rougeL_recall': tensor(0.3147)}, 'bertscore_': {'precision': 0.7949241353000732, 'recall': 0.778746617601273, 'f1': 0.7836252206817587, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: SZ Scene 2: {'rouge_score_': {'rouge1_fmeasure': tensor(0.3717), 'rouge1_precision': tensor(0.4029), 'rouge1_recall': tensor(0.4032), 'rougeL_fmeasure': tensor(0.3597), 'rougeL_precision': tensor(0.3894), 'rougeL_recall': tensor(0.3906)}, 'bertscore_': {'precision': 0.7897600459248832, 'recall': 0.7881977136215467, 'f1': 0.7869005366657557, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}


Evaluation results of: BD Scene 1: {'rouge_score_': {'rouge1_fmeasure': tensor(0.3208), 'rouge1_precision': tensor(0.3563), 'rouge1_recall': tensor(0.3479), 'rougeL_fmeasure': tensor(0.3145), 'rougeL_precision': tensor(0.3492), 'rougeL_recall': tensor(0.3410)}, 'bertscore_': {'precision': 0.7937158891132899, 'recall': 0.7880509906885576, 'f1': 0.7880669818443506, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: BD Scene 2: {'rouge_score_': {'rouge1_fmeasure': tensor(0.3785), 'rouge1_precision': tensor(0.4196), 'rouge1_recall': tensor(0.4230), 'rougeL_fmeasure': tensor(0.3630), 'rougeL_precision': tensor(0.4022), 'rougeL_recall': tensor(0.4070)}, 'bertscore_': {'precision': 0.7875163343189091, 'recall': 0.7875661454324064, 'f1': 0.7848666657147736, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: HC Scene 1: {'rouge_score_': {'rouge1_fmeasure': tensor(0.3136), 'rouge1_precision': tensor(0.3581), 'rouge1_recall': tensor(0.3460), 'rougeL_fmeasure': tensor(0.3054), 'rougeL_precision': tensor(0.3484), 'rougeL_recall': tensor(0.3378)}, 'bertscore_': {'precision': 0.7978556488835534, 'recall': 0.7913622420887615, 'f1': 0.7917972826680472, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: HC Scene 2: {'rouge_score_': {'rouge1_fmeasure': tensor(0.3818), 'rouge1_precision': tensor(0.4063), 'rouge1_recall': tensor(0.3943), 'rougeL_fmeasure': tensor(0.3603), 'rougeL_precision': tensor(0.3836), 'rougeL_recall': tensor(0.3719)}, 'bertscore_': {'precision': 0.7918783352098752, 'recall': 0.7895087182777634, 'f1': 0.7895826464558876, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: SZ Scene 1: {'rouge_score_': {'rouge1_fmeasure': tensor(0.2796), 'rouge1_precision': tensor(0.3366), 'rouge1_recall': tensor(0.3002), 'rougeL_fmeasure': tensor(0.2726), 'rougeL_precision': tensor(0.3275), 'rougeL_recall': tensor(0.2932)}, 'bertscore_': {'precision': 0.791280006157591, 'recall': 0.7754424488607873, 'f1': 0.780214362480539, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: SZ Scene 2: {'rouge_score_': {'rouge1_fmeasure': tensor(0.3549), 'rouge1_precision': tensor(0.3848), 'rouge1_recall': tensor(0.3782), 'rougeL_fmeasure': tensor(0.3445), 'rougeL_precision': tensor(0.3737), 'rougeL_recall': tensor(0.3669)}, 'bertscore_': {'precision': 0.7857739269063714, 'recall': 0.7827018556969889, 'f1': 0.7822723016310273, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}