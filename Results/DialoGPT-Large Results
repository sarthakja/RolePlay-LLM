Evaluation results of: 
BD Scene 1: {'rouge_score_': {'rouge1_fmeasure': (0.1580), 'rouge1_precision': (0.2111), 'rouge1_recall': (0.1541), 'rougeL_fmeasure': (0.1563), 'rougeL_precision': (0.2093), 'rougeL_recall': (0.1524)}, 'bertscore_': {'precision': 0.4521520894141834, 'recall': 0.4294867304688326, 'f1': 0.4386284742975063, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.38.2)'}}
Evaluation results of: BD Scene 2: {'rouge_score_': {'rouge1_fmeasure': (0.2398), 'rouge1_precision': (0.2855), 'rouge1_recall': (0.2456), 'rougeL_fmeasure': (0.2340), 'rougeL_precision': (0.2788), 'rougeL_recall': (0.2400)}, 'bertscore_': {'precision': 0.5652595031392443, 'recall': 0.5499847400319445, 'f1': 0.5554614240651602, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.38.2)'}}
Evaluation results of: HC Scene 1: {'rouge_score_': {'rouge1_fmeasure': (0.1446), 'rouge1_precision': (0.1877), 'rouge1_recall': (0.1540), 'rougeL_fmeasure': (0.1444), 'rougeL_precision': (0.1875), 'rougeL_recall': (0.1538)}, 'bertscore_': {'precision': 0.42215238955636686, 'recall': 0.40834085929472713, 'f1': 0.41321475537407476, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.38.2)'}}
Evaluation results of: HC Scene 2: {'rouge_score_': {'rouge1_fmeasure': (0.2605), 'rouge1_precision': (0.2920), 'rouge1_recall': (0.2776), 'rougeL_fmeasure': (0.2531), 'rougeL_precision': (0.2836), 'rougeL_recall': (0.2706)}, 'bertscore_': {'precision': 0.5791063733258337, 'recall': 0.569946657936528, 'f1': 0.572729656156504, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.38.2)'}}
Evaluation results of: SZ Scene 1: {'rouge_score_': {'rouge1_fmeasure': (0.1358), 'rouge1_precision': (0.1763), 'rouge1_recall': (0.1427), 'rougeL_fmeasure': (0.1326), 'rougeL_precision': (0.1727), 'rougeL_recall': (0.1393)}, 'bertscore_': {'precision': 0.4418114274740219, 'recall': 0.4285250911489129, 'f1': 0.43325912370346487, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.38.2)'}}
Evaluation results of: SZ Scene 2: {'rouge_score_': {'rouge1_fmeasure': (0.2630), 'rouge1_precision': (0.3004), 'rouge1_recall': (0.2686), 'rougeL_fmeasure': (0.2549), 'rougeL_precision': (0.2913), 'rougeL_recall': (0.2606)}, 'bertscore_': {'precision': 0.571870983562656, 'recall': 0.5555064544997401, 'f1': 0.561814283691971, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.38.2)'}}
