dialogpt_large:Evaluation results of: BD Scene 1: {'rouge_score_': {'rouge1_fmeasure': tensor(0.2610), 'rouge1_precision': tensor(0.3717), 'rouge1_recall': tensor(0.2552), 'rougeL_fmeasure': tensor(0.2591), 'rougeL_precision': tensor(0.3697), 'rougeL_recall': tensor(0.2532)}, 'bertscore_': {'precision': 0.8030060258041434, 'recall': 0.7467574760621908, 'f1': 0.7705148262231528, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: BD Scene 2: {'rouge_score_': {'rouge1_fmeasure': tensor(0.2548), 'rouge1_precision': tensor(0.3347), 'rouge1_recall': tensor(0.2663), 'rougeL_fmeasure': tensor(0.2460), 'rougeL_precision': tensor(0.3237), 'rougeL_recall': tensor(0.2578)}, 'bertscore_': {'precision': 0.7679131835185248, 'recall': 0.7347201683783325, 'f1': 0.7474989070203798, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: HC Scene 1: {'rouge_score_': {'rouge1_fmeasure': tensor(0.2908), 'rouge1_precision': tensor(0.3893), 'rouge1_recall': tensor(0.2831), 'rougeL_fmeasure': tensor(0.2889), 'rougeL_precision': tensor(0.3867), 'rougeL_recall': tensor(0.2804)}, 'bertscore_': {'precision': 0.8165519613166188, 'recall': 0.7652303192504617, 'f1': 0.787264930647473, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: HC Scene 2: {'rouge_score_': {'rouge1_fmeasure': tensor(0.2480), 'rouge1_precision': tensor(0.3091), 'rouge1_recall': tensor(0.2438), 'rougeL_fmeasure': tensor(0.2351), 'rougeL_precision': tensor(0.2943), 'rougeL_recall': tensor(0.2310)}, 'bertscore_': {'precision': 0.7730915515719565, 'recall': 0.7315415908850313, 'f1': 0.7492191474325155, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: SZ Scene 1: {'rouge_score_': {'rouge1_fmeasure': tensor(0.2429), 'rouge1_precision': tensor(0.3658), 'rouge1_recall': tensor(0.2336), 'rougeL_fmeasure': tensor(0.2409), 'rougeL_precision': tensor(0.3627), 'rougeL_recall': tensor(0.2319)}, 'bertscore_': {'precision': 0.7957553267478943, 'recall': 0.7406627235418939, 'f1': 0.7635535623164887, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: SZ Scene 2: {'rouge_score_': {'rouge1_fmeasure': tensor(0.2580), 'rouge1_precision': tensor(0.3446), 'rouge1_recall': tensor(0.2668), 'rougeL_fmeasure': tensor(0.2524), 'rougeL_precision': tensor(0.3384), 'rougeL_recall': tensor(0.2608)}, 'bertscore_': {'precision': 0.7700242560901, 'recall': 0.734158243222183, 'f1': 0.7484996703233612, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}