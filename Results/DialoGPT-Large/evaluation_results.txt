dialogpt_small:
Evaluation results of: BD Scene 1: {'rouge_score_': {'rouge1_fmeasure': tensor(0.2035), 'rouge1_precision': tensor(0.3193), 'rouge1_recall': tensor(0.1886), 'rougeL_fmeasure': tensor(0.2024), 'rougeL_precision': tensor(0.3178), 'rougeL_recall': tensor(0.1876)}, 'bertscore_': {'precision': 0.7991428552841654, 'recall': 0.7209090181759426, 'f1': 0.754602722164725, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: BD Scene 2: {'rouge_score_': {'rouge1_fmeasure': tensor(0.2085), 'rouge1_precision': tensor(0.2978), 'rouge1_recall': tensor(0.2067), 'rougeL_fmeasure': tensor(0.2047), 'rougeL_precision': tensor(0.2936), 'rougeL_recall': tensor(0.2029)}, 'bertscore_': {'precision': 0.7623239921983974, 'recall': 0.7049856257849726, 'f1': 0.7290722540226476, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: HC Scene 1: {'rouge_score_': {'rouge1_fmeasure': tensor(0.2220), 'rouge1_precision': tensor(0.3348), 'rouge1_recall': tensor(0.2067), 'rougeL_fmeasure': tensor(0.2214), 'rougeL_precision': tensor(0.3337), 'rougeL_recall': tensor(0.2063)}, 'bertscore_': {'precision': 0.8110302615997403, 'recall': 0.7368084515250006, 'f1': 0.769406882413598, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: HC Scene 2: {'rouge_score_': {'rouge1_fmeasure': tensor(0.1756), 'rouge1_precision': tensor(0.2614), 'rouge1_recall': tensor(0.1694), 'rougeL_fmeasure': tensor(0.1701), 'rougeL_precision': tensor(0.2554), 'rougeL_recall': tensor(0.1633)}, 'bertscore_': {'precision': 0.7554036442813955, 'recall': 0.6836932398730593, 'f1': 0.7146730205531796, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: SZ Scene 1: {'rouge_score_': {'rouge1_fmeasure': tensor(0.1936), 'rouge1_precision': tensor(0.3292), 'rouge1_recall': tensor(0.1804), 'rougeL_fmeasure': tensor(0.1928), 'rougeL_precision': tensor(0.3279), 'rougeL_recall': tensor(0.1794)}, 'bertscore_': {'precision': 0.7892486319104408, 'recall': 0.7180962813027362, 'f1': 0.7481501065194607, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: SZ Scene 2: {'rouge_score_': {'rouge1_fmeasure': tensor(0.1960), 'rouge1_precision': tensor(0.2945), 'rouge1_recall': tensor(0.1931), 'rougeL_fmeasure': tensor(0.1931), 'rougeL_precision': tensor(0.2910), 'rougeL_recall': tensor(0.1903)}, 'bertscore_': {'precision': 0.7646306948715381, 'recall': 0.7044928676626655, 'f1': 0.7301671723971206, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}