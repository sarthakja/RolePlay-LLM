Final result of BD Scene1:  {'rouge_score_': {'rouge1_fmeasure': tensor(0.0784), 'rouge1_precision': tensor(0.0661), 'rouge1_recall': tensor(0.1514), 'rougeL_fmeasure': tensor(0.0704), 'rougeL_precision': tensor(0.0594), 'rougeL_recall': tensor(0.1388)}, 'bertscore_': {'precision': 0.6973117027794842, 'recall': 0.7003776703285396, 'f1': 0.6880803272827437, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.38.2)'}}

Final result of BD Scene2:  {'rouge_score_': {'rouge1_fmeasure': tensor(0.1016), 'rouge1_precision': tensor(0.0914), 'rouge1_recall': tensor(0.1579), 'rougeL_fmeasure': tensor(0.0834), 'rougeL_precision': tensor(0.0740), 'rougeL_recall': tensor(0.1351)}, 'bertscore_': {'precision': 0.6894358175468969, 'recall': 0.685079785329955, 'f1': 0.6784594635387043, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.38.2)'}}

Final result of HC Scene1:  {'rouge_score_': {'rouge1_fmeasure': tensor(0.0771), 'rouge1_precision': tensor(0.0668), 'rouge1_recall': tensor(0.1326), 'rougeL_fmeasure': tensor(0.0691), 'rougeL_precision': tensor(0.0595), 'rougeL_recall': tensor(0.1207)}, 'bertscore_': {'precision': 0.6991705393159626, 'recall': 0.6997779035410344, 'f1': 0.6887767530040235, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.38.2)'}}

Final result of HC Scene2:  {'rouge_score_': {'rouge1_fmeasure': tensor(0.1164), 'rouge1_precision': tensor(0.1074), 'rouge1_recall': tensor(0.1680), 'rougeL_fmeasure': tensor(0.0991), 'rougeL_precision': tensor(0.0911), 'rougeL_recall': tensor(0.1475)}, 'bertscore_': {'precision': 0.6805651550585369, 'recall': 0.6975358711942187, 'f1': 0.6834338894992504, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.38.2)'}}

Final result of SZ Scene1:  {'rouge_score_': {'rouge1_fmeasure': tensor(0.0754), 'rouge1_precision': tensor(0.0645), 'rouge1_recall': tensor(0.1620), 'rougeL_fmeasure': tensor(0.0679), 'rougeL_precision': tensor(0.0584), 'rougeL_recall': tensor(0.1501)}, 'bertscore_': {'precision': 0.6957791456370614, 'recall': 0.7063803874771111, 'f1': 0.6895255984272808, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.38.2)'}}

Final result of SZ Scene2:  {'rouge_score_': {'rouge1_fmeasure': tensor(0.1160), 'rouge1_precision': tensor(0.1098), 'rouge1_recall': tensor(0.1644), 'rougeL_fmeasure': tensor(0.0951), 'rougeL_precision': tensor(0.0891), 'rougeL_recall': tensor(0.1395)}, 'bertscore_': {'precision': 0.6841780469237759, 'recall': 0.687230167691934, 'f1': 0.6780975226250441, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.38.2)'}}
