t5base: 
Evaluation results of: BD Scene 1: {'rouge_score_': {'rouge1_fmeasure': tensor(0.1368), 'rouge1_precision': tensor(0.2121), 'rouge1_recall': tensor(0.1308), 'rougeL_fmeasure': tensor(0.1360), 'rougeL_precision': tensor(0.2112), 'rougeL_recall': tensor(0.1301)}, 'bertscore_': {'precision': 0.7970112571505462, 'recall': 0.7139221777315854, 'f1': 0.7501354370798383, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: BD Scene 2: {'rouge_score_': {'rouge1_fmeasure': tensor(0.1169), 'rouge1_precision': tensor(0.2051), 'rouge1_recall': tensor(0.1092), 'rougeL_fmeasure': tensor(0.1158), 'rougeL_precision': tensor(0.2040), 'rougeL_recall': tensor(0.1080)}, 'bertscore_': {'precision': 0.7574198984637343, 'recall': 0.6635310865039455, 'f1': 0.7035681895654777, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: HC Scene 1: {'rouge_score_': {'rouge1_fmeasure': tensor(0.1600), 'rouge1_precision': tensor(0.2384), 'rouge1_recall': tensor(0.1466), 'rougeL_fmeasure': tensor(0.1594), 'rougeL_precision': tensor(0.2376), 'rougeL_recall': tensor(0.1460)}, 'bertscore_': {'precision': 0.8097000698710597, 'recall': 0.7273316955150559, 'f1': 0.7638572264549344, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: HC Scene 2: {'rouge_score_': {'rouge1_fmeasure': tensor(0.1241), 'rouge1_precision': tensor(0.2069), 'rouge1_recall': tensor(0.1166), 'rougeL_fmeasure': tensor(0.1196), 'rougeL_precision': tensor(0.2009), 'rougeL_recall': tensor(0.1127)}, 'bertscore_': {'precision': 0.752932708406653, 'recall': 0.6702205932703141, 'f1': 0.7054806538405848, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: SZ Scene 1: {'rouge_score_': {'rouge1_fmeasure': tensor(0.1289), 'rouge1_precision': tensor(0.2395), 'rouge1_recall': tensor(0.1171), 'rougeL_fmeasure': tensor(0.1282), 'rougeL_precision': tensor(0.2386), 'rougeL_recall': tensor(0.1164)}, 'bertscore_': {'precision': 0.783874446486539, 'recall': 0.7011101085375598, 'f1': 0.7364428861344114, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: SZ Scene 2: {'rouge_score_': {'rouge1_fmeasure': tensor(0.1263), 'rouge1_precision': tensor(0.2228), 'rouge1_recall': tensor(0.1183), 'rougeL_fmeasure': tensor(0.1240), 'rougeL_precision': tensor(0.2204), 'rougeL_recall': tensor(0.1159)}, 'bertscore_': {'precision': 0.7492008231998829, 'recall': 0.6716032984551419, 'f1': 0.7046781883480844, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}


Evaluation results of: BD Scene 1: {'rouge_score_': {'rouge1_fmeasure': tensor(0.1531), 'rouge1_precision': tensor(0.2412), 'rouge1_recall': tensor(0.1441), 'rougeL_fmeasure': tensor(0.1520), 'rougeL_precision': tensor(0.2400), 'rougeL_recall': tensor(0.1431)}, 'bertscore_': {'precision': 0.8011154218595855, 'recall': 0.716047466125618, 'f1': 0.7532025928400001, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: BD Scene 2: {'rouge_score_': {'rouge1_fmeasure': tensor(0.1395), 'rouge1_precision': tensor(0.2476), 'rouge1_recall': tensor(0.1315), 'rougeL_fmeasure': tensor(0.1377), 'rougeL_precision': tensor(0.2458), 'rougeL_recall': tensor(0.1295)}, 'bertscore_': {'precision': 0.7670068701022658, 'recall': 0.6722729453367406, 'f1': 0.7127088860704981, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: HC Scene 1: {'rouge_score_': {'rouge1_fmeasure': tensor(0.1768), 'rouge1_precision': tensor(0.2531), 'rouge1_recall': tensor(0.1608), 'rougeL_fmeasure': tensor(0.1762), 'rougeL_precision': tensor(0.2524), 'rougeL_recall': tensor(0.1602)}, 'bertscore_': {'precision': 0.8145541354667308, 'recall': 0.7318123739819195, 'f1': 0.768717800877815, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: HC Scene 2: {'rouge_score_': {'rouge1_fmeasure': tensor(0.1207), 'rouge1_precision': tensor(0.2066), 'rouge1_recall': tensor(0.1099), 'rougeL_fmeasure': tensor(0.1172), 'rougeL_precision': tensor(0.2023), 'rougeL_recall': tensor(0.1069)}, 'bertscore_': {'precision': 0.7533180619002412, 'recall': 0.6689245936184993, 'f1': 0.7050136954487649, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: SZ Scene 1: {'rouge_score_': {'rouge1_fmeasure': tensor(0.1493), 'rouge1_precision': tensor(0.2682), 'rouge1_recall': tensor(0.1373), 'rougeL_fmeasure': tensor(0.1481), 'rougeL_precision': tensor(0.2670), 'rougeL_recall': tensor(0.1361)}, 'bertscore_': {'precision': 0.787317851360174, 'recall': 0.7074074939368887, 'f1': 0.7413435590235476, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: SZ Scene 2: {'rouge_score_': {'rouge1_fmeasure': tensor(0.1210), 'rouge1_precision': tensor(0.2233), 'rouge1_recall': tensor(0.1154), 'rougeL_fmeasure': tensor(0.1183), 'rougeL_precision': tensor(0.2206), 'rougeL_recall': tensor(0.1123)}, 'bertscore_': {'precision': 0.7557518460777368, 'recall': 0.673623409833801, 'f1': 0.7087389880351807, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}
