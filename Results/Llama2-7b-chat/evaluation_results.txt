Evaluation results of: BD Scene 1: {'rouge_score_': {'rouge1_fmeasure': tensor(0.1330), 'rouge1_precision': tensor(0.1040), 'rouge1_recall': tensor(0.3069), 'rougeL_fmeasure': tensor(0.1191), 'rougeL_precision': tensor(0.0930), 'rougeL_recall': tensor(0.2825)}, 'bertscore_': {'precision': 0.6505585086531938, 'recall': 0.7446678983047604, 'f1': 0.6923751417547465, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}Evaluation results of: BD Scene 2: {'rouge_score_': {'rouge1_fmeasure': tensor(0.1444), 'rouge1_precision': tensor(0.1161), 'rouge1_recall': tensor(0.2687), 'rougeL_fmeasure': tensor(0.1128), 'rougeL_precision': tensor(0.0894), 'rougeL_recall': tensor(0.2245)}, 'bertscore_': {'precision': 0.6547430692952079, 'recall': 0.7245252270555291, 'f1': 0.6864164548370459, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}Evaluation results of: HC Scene 1: {'rouge_score_': {'rouge1_fmeasure': tensor(0.1296), 'rouge1_precision': tensor(0.0992), 'rouge1_recall': tensor(0.3102), 'rougeL_fmeasure': tensor(0.1132), 'rougeL_precision': tensor(0.0862), 'rougeL_recall': tensor(0.2793)}, 'bertscore_': {'precision': 0.6535633037518421, 'recall': 0.7462764679462917, 'f1': 0.6948914980002601, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}Evaluation results of: HC Scene 2: {'rouge_score_': {'rouge1_fmeasure': tensor(0.1518), 'rouge1_precision': tensor(0.1294), 'rouge1_recall': tensor(0.2551), 'rougeL_fmeasure': tensor(0.1193), 'rougeL_precision': tensor(0.1003), 'rougeL_recall': tensor(0.2122)}, 'bertscore_': {'precision': 0.6568342700396499, 'recall': 0.7244030291631341, 'f1': 0.687319539453341, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}Evaluation results of: SZ Scene 1: {'rouge_score_': {'rouge1_fmeasure': tensor(0.1124), 'rouge1_precision': tensor(0.0881), 'rouge1_recall': tensor(0.2979), 'rougeL_fmeasure': tensor(0.1024), 'rougeL_precision': tensor(0.0801), 'rougeL_recall': tensor(0.2800)}, 'bertscore_': {'precision': 0.6443866879766823, 'recall': 0.7434141947927638, 'f1': 0.688059984151794, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}Evaluation results of: SZ Scene 2: {'rouge_score_': {'rouge1_fmeasure': tensor(0.1490), 'rouge1_precision': tensor(0.1184), 'rouge1_recall': tensor(0.2596), 'rougeL_fmeasure': tensor(0.1170), 'rougeL_precision': tensor(0.0915), 'rougeL_recall': tensor(0.2134)}, 'bertscore_': {'precision': 0.654041388976841, 'recall': 0.722763947095502, 'f1': 0.6853786245625763, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}Evaluation results of: BD Scene 1: {'rouge_score_': {'rouge1_fmeasure': tensor(0.1258), 'rouge1_precision': tensor(0.0988), 'rouge1_recall': tensor(0.3363), 'rougeL_fmeasure': tensor(0.1123), 'rougeL_precision': tensor(0.0885), 'rougeL_recall': tensor(0.3069)}, 'bertscore_': {'precision': 0.639401586074382, 'recall': 0.7445943932048976, 'f1': 0.6853579800575972, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}Evaluation results of: BD Scene 2: {'rouge_score_': {'rouge1_fmeasure': tensor(0.1212), 'rouge1_precision': tensor(0.0837), 'rouge1_recall': tensor(0.3673), 'rougeL_fmeasure': tensor(0.0924), 'rougeL_precision': tensor(0.0638), 'rougeL_recall': tensor(0.2981)}, 'bertscore_': {'precision': 0.6195195473954401, 'recall': 0.7276517527553656, 'f1': 0.6673340931087093, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}Evaluation results of: HC Scene 1: {'rouge_score_': {'rouge1_fmeasure': tensor(0.1200), 'rouge1_precision': tensor(0.0926), 'rouge1_recall': tensor(0.3243), 'rougeL_fmeasure': tensor(0.1044), 'rougeL_precision': tensor(0.0808), 'rougeL_recall': tensor(0.2905)}, 'bertscore_': {'precision': 0.6425484698438793, 'recall': 0.7441080624105022, 'f1': 0.68709023449074, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}Evaluation results of: HC Scene 2: {'rouge_score_': {'rouge1_fmeasure': tensor(0.1222), 'rouge1_precision': tensor(0.0884), 'rouge1_recall': tensor(0.3371), 'rougeL_fmeasure': tensor(0.0923), 'rougeL_precision': tensor(0.0655), 'rougeL_recall': tensor(0.2739)}, 'bertscore_': {'precision': 0.6227297131057199, 'recall': 0.727409853782828, 'f1': 0.6689730452620275, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}Evaluation results of: SZ Scene 1: {'rouge_score_': {'rouge1_fmeasure': tensor(0.1099), 'rouge1_precision': tensor(0.0854), 'rouge1_recall': tensor(0.3226), 'rougeL_fmeasure': tensor(0.0994), 'rougeL_precision': tensor(0.0775), 'rougeL_recall': tensor(0.2983)}, 'bertscore_': {'precision': 0.6325420612453395, 'recall': 0.7449450095919122, 'f1': 0.6813723544279734, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}Evaluation results of: SZ Scene 2: {'rouge_score_': {'rouge1_fmeasure': tensor(0.1217), 'rouge1_precision': tensor(0.0835), 'rouge1_recall': tensor(0.3399), 'rougeL_fmeasure': tensor(0.0927), 'rougeL_precision': tensor(0.0635), 'rougeL_recall': tensor(0.2702)}, 'bertscore_': {'precision': 0.6193510673925008, 'recall': 0.7255958281078067, 'f1': 0.666644444837104, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}
