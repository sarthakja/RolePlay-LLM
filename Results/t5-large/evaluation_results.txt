t5-large: Evaluation results of: BD Scene 1: {'rouge_score_': {'rouge1_fmeasure': tensor(0.2058), 'rouge1_precision': tensor(0.3017), 'rouge1_recall': tensor(0.1943), 'rougeL_fmeasure': tensor(0.2043), 'rougeL_precision': tensor(0.3000), 'rougeL_recall': tensor(0.1929)}, 'bertscore_': {'precision': 0.7988244568815037, 'recall': 0.7299785871084045, 'f1': 0.759534544158144, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: BD Scene 2: {'rouge_score_': {'rouge1_fmeasure': tensor(0.1885), 'rouge1_precision': tensor(0.3025), 'rouge1_recall': tensor(0.1821), 'rougeL_fmeasure': tensor(0.1859), 'rougeL_precision': tensor(0.2994), 'rougeL_recall': tensor(0.1793)}, 'bertscore_': {'precision': 0.7784649558879179, 'recall': 0.695843426189546, 'f1': 0.7314422076632237, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: HC Scene 1: {'rouge_score_': {'rouge1_fmeasure': tensor(0.2348), 'rouge1_precision': tensor(0.3375), 'rouge1_recall': tensor(0.2219), 'rougeL_fmeasure': tensor(0.2332), 'rougeL_precision': tensor(0.3354), 'rougeL_recall': tensor(0.2205)}, 'bertscore_': {'precision': 0.8126095389210901, 'recall': 0.7519520315319993, 'f1': 0.7782803693483042, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: HC Scene 2: {'rouge_score_': {'rouge1_fmeasure': tensor(0.2101), 'rouge1_precision': tensor(0.2976), 'rouge1_recall': tensor(0.2029), 'rougeL_fmeasure': tensor(0.2039), 'rougeL_precision': tensor(0.2897), 'rougeL_recall': tensor(0.1972)}, 'bertscore_': {'precision': 0.7712214256560853, 'recall': 0.7098054215631772, 'f1': 0.7360785398872114, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: SZ Scene 1: {'rouge_score_': {'rouge1_fmeasure': tensor(0.1996), 'rouge1_precision': tensor(0.3323), 'rouge1_recall': tensor(0.1830), 'rougeL_fmeasure': tensor(0.1973), 'rougeL_precision': tensor(0.3291), 'rougeL_recall': tensor(0.1809)}, 'bertscore_': {'precision': 0.7960571503227062, 'recall': 0.7227525832725966, 'f1': 0.7537731355492104, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}

Evaluation results of: SZ Scene 2: {'rouge_score_': {'rouge1_fmeasure': tensor(0.1903), 'rouge1_precision': tensor(0.2953), 'rouge1_recall': tensor(0.1843), 'rougeL_fmeasure': tensor(0.1867), 'rougeL_precision': tensor(0.2912), 'rougeL_recall': tensor(0.1805)}, 'bertscore_': {'precision': 0.7742960494555784, 'recall': 0.7015838508525591, 'f1': 0.7328260916002681, 'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.42.3)'}}
